![Workshop-logo](../IM/LOGO_new.png)
# Applications of Genomics in Wildlife Conservation

## Day 1 - Session 1 - Basic concepts of command line programming

### 1. Working from terminal and remote servers
In this section we will revise the procedure to connect to our cloud server and we will provide you with some preliminary definitions.


### 2. The filesystem 
In this section we will revise basic commands to navigate filesystems and create a series of directories and sub-directories for a research project. Naming file and directory is more important than you may think; a clear and consistent structure will make your life way much easier when you’ll have to deal with complex projects and pipelines. Feel free to adjust and customize this template structure as you please but keep this criteria in mind.

We should start with creating a parent directory for our project. 
```sh
mkdir Project_BASH 
cd Project_BASH
```
The first command (`mkdir`) creates the required folder while the `cd` command is used to move from your home directory to the newly created one. Now we need to create one directory where we are going to store our scripts, one for the raw data, and finally one for our results:

```sh
mkdir Scripts; mkdir Raw_Data; mkdir Results
```
note the use of the semicolon (;) to separate different commands on the same line.
> `Hacker Tips`: never use white spaces when naming files or directories

Then, we need to create a separate directory for each stage of our pipeline/analysis. Because of my lack of immagination, each directory will be termed as `Stage` and we are going to number them sequentially but I would encourage you to use a better naming
convention when it comes to your research. Given that typing the same command over and over is tedious, we are going to use one of the basics building blocks of any programming language: a loop.  

```sh
for i in $(seq 3)
do
mkdir Stage_$i
done
```
Let's unpack this programming structure:

The reserved word `for` is what signals the shell that the following commands need to be repeated.

The letter `i` is our iterator, the loop will stop when this variable has taken all the values in the array generated by the command `seq`. As you have probably figured out already, `seq 3` will generate a sequence of integers from 1 to 3 (included).  

In order to make this sequence available to the for loop, so that the machine will no when to stop, we use the bash syntax `$(command)` which roughly speaking means "insert here the output of the command in brackets". 

We then used the iterator (i) to name our directories. Because at each iteration i is going to assume a different value, we used the standard notation for variables in our mkdir command: $i.

Now let's look at what we have done: to list the content of a directory use the command `ls`.

For the sake of reproducibility, it is always a good idea to keep track of everything we do. I recommend you always log your command to a file keeping a consitent naming convention. Here we are going to call our log files `What_I_did.txt` and in order to create such files we will use the command `touch`. 

Each of our `Stage` directories should contain at least three items:
- a What_I_Did.txt file.
- an Output sub-directory
- a sub-directory called Input

In principles, you could navigate to the each Stage directory using the `cd` command and create these objects manually but that involves a lot of typing. You should instead use a loop to avoid this tedious task. An efficient way to do this from a shell terminal requires to create a list of parent directories and then create the child directories only where we need them.
```sh
ls -d Stage* > dir_list.txt
```
Note the use of the `>`. This symbol in bash has a special meaning: redirect the output of the command that precedes it to a file.

Now that we have the list, we can easily implement another type of loop using the reserved word `while`:
```sh
while read -r line
do
touch $line/What_I_Did.txt
done < dir_list.txt
```
note again the use of the `$` sign in front of the word `line` which is the syntax for variables in bash. This is because at each iteration the content of the variable `line` will change while we go through the list of directories contained in the `dir_list.txt` file. the loop will end when the variable `$line` is empty i.e. when we reach the end of the file.

> `Exercise 1`
>
> use a similar while loop to create a sub-directory called `Output` inside each Stage dir

If your pipeline is run sequentially, the output of the first step will serve as input for the second step and so on and so forth. Therefore, the content of `Stage 1/Output` and `Stage 2/Input` will be exactly the same. Avoiding redundancies and data duplication is a good way of saving space on disk though, thus, we don’t want to copy all the files from one directory to another: we are going to create a symbolic link instead.
```sh
ln -s ~/Project_BASH/Raw_Data/ ~/Project_BASH/Stage_1/Input
```
The `ln` command stands for ”link” and it has this general syntax:
```sh
ln full/path/to/source full/path/to/destination/link_name
```
Here we have used the `-s` flag to specify a symbolic link between the `Raw_Data` directory and a new `Input` folder (link name) inside the `Stage_1 directory`. This means that the content of `Raw_Data` is now accessible from `Stage_1/Input`. Let’s double-check it:
```sh
touch Raw_Data/input_zero.txt
ls Stage_1
ls Stage_1/Input
ls Raw_Data
```
The first command is just to pupulate the Raw_Data folder with a file (`input_zero.txt`). By running the second command you should see that an Input folder has been created via the `ln` command. Now the Stage_1 directory contains all three elements required. The output of the third and fourth commands should be just `input_zero.txt`.

> `Exercise 2`
>
> Use the ln command to complete our file system structure
> by linking each `Stage_{i}/Output` to an Input folder inside the `Stage_{i+1}` directory.
> Note that in our example Stage 4 is actually termed Results.

If you have made a mistake with links, do not panic. You can alwayse remove them with `rm` or with the `unlink` command.

Now that we have a good structure we can start populating our directories. Let's create some files in the `Project_BASH` directory.

```sh
for i in $(seq 3)
do
touch output_file_$1
done
```
Now we should move each of these file to the corresponding `Stage_{i}/Output` directory.

> `Exercise 3`
>
> Use a for foop to move each of the output_file_{i} ∀i ∈ {1, 2, 3} to its own directory.
> 
> To do so, you should use the `mv` command which has the following syntax: `mv target_file_name path/to/destination`.


If you now run the `tree` command from the Project_BASH directory, you should get:

![File-system-structure](../IM/bash_tree.png)

### 3. Regular expressions & file manipulations
In this section we are going to apply what we already know about variables, conditional and loops to explore text files. 
We will mostly focus on file manipulation using regular expressions, using `grep`, `sed` and `awk`.

Let’s start with opening a new terminal, connecting to the VM and look at the file called `random.fasta`:
```sh
cd /home/Data/Day_1/
less random.fasta
Press Q to exit.
```

As the name suggests, this file contains some random DNA sequences of different length stored in a commonly used format for this kind of data. 
As you can see, each entry consists of two lines: a header (with the sequence identifier) and a second line containing the actual sequence.
Thus, if we were wondering how many DNA sequences are present in that file, we can just run:
```sh
wc -l random.fasta
```
and divide this number by two.

Now, let’s look at the first 7 entries which correspond to the first 14 lines of the file.
We can easily print them on screen using:
```sh
head -14 random.fasta
```

If instead we were interested in the last 4 entries, we could have run:
```sh
tail -8 random.fasta
```
Or we could a combination of these two commands to extract a contiguous block of sequence:
```sh
head -20 random.fasta | tail -4
```
With the last command we have selected the 9th and 10th entries corresponding to the lines from 17 to 20 in our fasta file.
The vertical bar (`|`) is called `pipe` and it is used to connect the two commands (`head` and `tail` in this case). 
Specifically, it redirects the standard output of the first command which then serves as input for the second command.

Piping the output of head into tail or vice versa is a simple way to extract a block of lines
but it becomes very slow if the file you are dealing with is huge. An alternative is using
sed. Let’s consider the following commands:

```sh
sed -n '9,12p' random.fasta
sed -n '9,+3p' random.fasta
```
These are alternative ways of printing a range of lines. In this case we are printing lines
from 9 to 12 which of course correspond to our 5th and 6th entry.

It is unlikely though that we will know in advance the line numbers of the entries that are
relevant to our analysis. Most of the time will have to parse the file and look for patterns.
That’s when regular expressions (`regex`) become very useful.

As you may have noticed, all header lines in random.fasta start with `>seq` followed by a
number, `Hg`, and a letter, separated by underscores. The string `Hg` stands for haplogroup
(A,B,or C) and we might be interested in knowing how many reads we have for each
group. We can retrieve these number by running:

```sh
grep '>seq.*_Hg_A' random.fasta | wc -l
```
Let's unpack this command:

`grep` stands for "global regular expression print" and it has the following syntax
```sh
grep 'regex' target_file
```
In our example the regex or the pattern that we are looking is obviously `>seq.*_Hg_A`. 
This regular expression is designed not to match a unique sequence of character but rather a series of sequences that are similar to each another.
In fact, this regex will match any line containing the string `>seq` followed by any character (represented by the `.` symbol) that appears zero or more times (`*`), 
followed by the string `_Hg_A`. Thus, `grep` will print all the header lines in the random.fasta corresponding to haplogroup A and this output is the piped (|) 
into the command `wc -l` which simply counts the number of matching lines.

> `Exercise 4`
>
> Find out how many sequences we have for each group by modifying the pattern of the grep command above.

We can also use regex inside a sed command. For example, let's extract the 3rd sequence of each haplogroup:
```sh
sed -n '/seq_3_/,+1p' random.fasta > /home/your-user-id/Project_BASH/Raw_Data/third_seq_all_Hg.fasta
```
As you can see, it looks very similar to the sed command we used before with the exception that instead of providing sed with specific line number, 
here we have specified a pattern (`/seq_3_/`) and asked the program to print eachh matching line plus and the following one: (+1p). 
Finally, we have redirected the output to store this information into a file called third_seq_all_Hg.fasta.
Now navigate to your `Raw_Data` directory and visualise the content of the file on screen using the command `cat name-of-the-file`.


Let’s have a look at a different file format and keep on experimenting with regex. 
In the`/home/Data/Day_1` folder you should see a file called `dog_genes.gtf`  
which is a tab separated file containing annotations for some coding sequences in the dog genome.
We can have a look at the header (lines starting with #) by running:
```sh
grep '^#' /home/Data/Day_1/dog_genes.gtf
```
> `Exercise 5`
>
> Create an headerless version of this file called dog_genes_no_H.tsv using the ”select non matching lines” option of grep (-v flag).



